{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 09:56:40.876999: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 09:56:41.611770: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using concept features\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import * \n",
    "from vis_utils import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(checkpoint_path, map_location=gpu)\n",
    "state_dict = ckpt['state_dict']\n",
    "state_dict = get_regular_ckpt_from_lightning_checkpoint(state_dict)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "model = model.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:3! (when checking argument for argument tensors in method wrapper_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m img, angle, distance, vego \u001b[39m=\u001b[39m image_array\u001b[39m.\u001b[39mto(gpu)[\u001b[39m0\u001b[39m:x], angle\u001b[39m.\u001b[39mto(gpu)[\u001b[39m0\u001b[39m:x], distance\u001b[39m.\u001b[39mto(gpu)[\u001b[39m0\u001b[39m:x], vego\u001b[39m.\u001b[39mto(gpu)[\u001b[39m0\u001b[39m:x]\n\u001b[1;32m     10\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m (logits, attns), concepts \u001b[39m=\u001b[39m model(img, angle, distance, vego)\n\u001b[1;32m     12\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     13\u001b[0m elapsed_time \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/personalized_driving_toyota/analyse_results/../model.py:195\u001b[0m, in \u001b[0;36mVTN.forward\u001b[0;34m(self, img, angle, distance, vego)\u001b[0m\n\u001b[1;32m    193\u001b[0m attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((B, D), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    194\u001b[0m cls_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token\u001b[39m.\u001b[39mexpand(B, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# stole cls_tokens impl from Phil Wang, thanks\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((cls_tokens, x), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    197\u001b[0m cls_atten \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand(B, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    198\u001b[0m attention_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((attention_mask, cls_atten), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:3! (when checking argument for argument tensors in method wrapper_cat)"
     ]
    }
   ],
   "source": [
    "num_iterations = 100  # Number of prediction iterations\n",
    "\n",
    "times = []\n",
    "xs = [50, 100, 150, 200, 240]\n",
    "for x in xs:\n",
    "    times = []\n",
    "    for j, batch in enumerate(dataloader_comma):\n",
    "        image_array,  vego, angle, distance, g, s, l = batch\n",
    "        img, angle, distance, vego = image_array.to(gpu)[0:x], angle.to(gpu)[0:x], distance.to(gpu)[0:x], vego.to(gpu)[0:x]\n",
    "        start_time = time.time()\n",
    "        (logits, attns), concepts = model(img, angle, distance, vego)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        times.append(elapsed_time)\n",
    "        #angle, distance, vego, logits, concepts = angle.to(\"cpu\"), distance.to(\"cpu\"), vego.to(\"cpu\"), logits.detach().cpu().to(\"cpu\"), concepts.detach().cpu().to(\"cpu\")\n",
    "\n",
    "    throughput = np.mean(times)\n",
    "    print(f\"Avg latency for {x}: {throughput:.2f} pps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_predictions = 0\n",
    "max_time = 1  # Time period in seconds\n",
    "\n",
    "tpss = []\n",
    "for i in range(100):\n",
    "    while time.time() - start_time < max_time:\n",
    "        image_array,  vego, angle, distance, g, s, l = next(iter(dataloader_comma))\n",
    "        img, angle, distance, vego = image_array.to(gpu), angle.to(gpu), distance.to(gpu), vego.to(gpu)\n",
    "        (logits, attns), concepts = model(img, angle, distance, vego)\n",
    "        num_predictions += 1\n",
    "\n",
    "    throughput = num_predictions / max_time\n",
    "    tpss.append(throughput)\n",
    "np.mean(tpss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "num_predictions = 0\n",
    "max_time = 1  # Time period in seconds\n",
    "tpss = []\n",
    "for i in range(100):\n",
    "    while time.time() - start_time < max_time:\n",
    "        _, image_array,  vego, angle, distance, _, _, _, _, _ = next(iter(dataloader_nuscenes))\n",
    "        img, angle, distance, vego = image_array.to(gpu), angle.to(gpu), distance.to(gpu), vego.to(gpu)\n",
    "        (logits, attns), concepts = model(img, angle, distance, vego)\n",
    "        num_predictions += 1\n",
    "\n",
    "    throughput = num_predictions / max_time\n",
    "    tpss.append(throughput)\n",
    "np.mean(tpss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction latency: 246.97 ms\n"
     ]
    }
   ],
   "source": [
    "image_array,  vego, angle, distance, g, s, l = next(iter(dataloader_comma))\n",
    "img, angle, distance, vego = img.to(gpu), angle.to(gpu), distance.to(gpu), vego.to(gpu)\n",
    "start_time = time.time()\n",
    "(logits, attns), concepts = model(img, angle, distance, vego)\n",
    "end_time = time.time()\n",
    "latency = (end_time - start_time) * 1000  # Convert to milliseconds\n",
    "print(f\"Prediction latency: {latency:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/home/jessica/personalized_driving_toyota/scenarios/scenarios_curated_270.txt'\n",
    "with open(p) as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "scenarios = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/home/jessica/personalized_driving_toyota/scenarios/scenarios_nuscenes.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(p) as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "scenarios2 = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [elem not in scenarios for elem in scenarios2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.array(scenarios2)[m]).sample(n=400).to_csv(\"scen2.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args1 = {'model_name_or_path': '/data4/jessica/data/toyota/baselines/bert-base-uncased/', \n",
    "        'config_name': '', 'tokenizer_name': '', 'num_hidden_layers': -1, \n",
    "        'hidden_size': -1, 'num_attention_heads': -1, 'intermediate_size': -1, \n",
    "        'img_feature_dim': 512, 'load_partial_weights': False, 'freeze_embedding': False, \n",
    "        'drop_out': 0.1, 'max_seq_length': 30, 'max_seq_a_length': 30, 'max_img_seq_length': 196, \n",
    "        'do_lower_case': True, 'add_od_labels': False, 'od_label_conf': 0.0, 'use_asr': False, \n",
    "        'use_sep_cap': False, 'use_swap_cap': False, 'use_car_sensor': False, 'multitask': False, \n",
    "        'only_signal': True, 'signal_types': ['course'], 'unique_labels_on': False, 'no_sort_by_conf': False,\n",
    "        'mask_prob': 0.5, 'max_masked_tokens': 45, 'attn_mask_type': 'seq2seq', 'text_mask_type': 'random', \n",
    "        'tag_to_mask': ['noun', 'verb'], 'mask_tag_prob': -1, 'random_mask_prob': 0, \n",
    "        'on_memory': False, 'effective_batch_size': 1, 'per_gpu_train_batch_size': 1, 'num_workers': 10, 'limited_samples': -1, 'learning_rate': 0.0003,\n",
    "         'weight_decay': 0.05, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'warmup_ratio': 0.1, 'scheduler': 'warmup_linear', 'gradient_accumulation_steps': 1, \n",
    "         'num_train_epochs': 1, 'logging_steps': 20, 'save_steps': 2000, 'restore_ratio': -1, 'device': f\"cuda:{args.gpu_num}\", 'seed': 88, 'local_rank': 0,\n",
    "          'mixed_precision_method': 'deepspeed', 'zero_opt_stage': -1, 'amp_opt_level': 0, 'deepspeed_fp16': True, 'fairscale_fp16': False, 'pretrained_checkpoint': '', \n",
    "          'debug': False, 'debug_speed': False, 'config': 'src/configs/VidSwinBert/BDDX_multi_default.json', 'eval_model_dir': '',\n",
    "        'do_train': True, 'do_test': False, 'do_eval': False, 'do_signal_eval': False, 'evaluate_during_training': True, \n",
    "          'per_gpu_eval_batch_size': 1, 'mask_img_feat': False, 'max_masked_img_tokens': 10, 'tie_weights': False, 'label_smoothing': 0, 'drop_worst_ratio': 0, \n",
    "          'drop_worst_after': 0, 'max_gen_length': 15, 'output_hidden_states': False, 'num_return_sequences': 1, 'num_beams': 1, 'num_keep_best': 1, 'temperature': 1, \n",
    "          'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'length_penalty': 1, 'use_cbs': False, 'min_constraints_to_satisfy': 2, 'use_hypo': False,\n",
    "           'decoding_constraint': False, 'remove_bad_endings': False, 'scst': False, 'sc_train_sample_n': 5, 'sc_baseline_type': 'greedy', 'cider_cached_tokens': \n",
    "           'coco_caption/gt/coco-train-words.p', 'max_num_frames': 8, 'img_res': 224, 'patch_size': 32, 'grid_feat': True, 'kinetics': '600', 'pretrained_2d': False, \n",
    "           'vidswin_size': 'base', 'freeze_backbone': False, 'use_checkpoint': True, 'backbone_coef_lr': 0.05, 'reload_pretrained_swin': False, 'learn_mask_enabled': False, \n",
    "           'loss_sparse_w': 0, 'loss_sensor_w': 0, 'sparse_mask_soft2hard': False, 'transfer_method': -1, 'att_mask_expansion': -1, 'resume_checkpoint': 'None', \n",
    "           'use_clip_model': True, 'num_gpus': 4, 'distributed': False}\n",
    "\n",
    "swin_model = get_swin_model(args1)\n",
    "bert_model, config, tokenizer = get_bert_model(args1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
